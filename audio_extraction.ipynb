{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj5QeSXHzItG+qToqZoKkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RANGAM-AKHILA/TESS/blob/main/audio_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCa6jnsDTgwv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moAJArfW9-xe"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torchaudio tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgPFLrVw_aj-",
        "outputId": "0c91d8fd-f204-41fb-a97b-1b30ac1b9aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'toronto-emotional-speech-set-tess' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDxef-W1_ees",
        "outputId": "8b9e898c-c857-406b-c0af-4e87db462ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/input/toronto-emotional-speech-set-tess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "DATASET_PATH = \"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data\"\n",
        "FEATURE_SAVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/hubert_features\"\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "os.makedirs(FEATURE_SAVE_PATH, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU0Lzt8l-LI3",
        "outputId": "1db565dd-5a50-4410-a93b-f4fbb63f670a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor, HubertModel\n",
        "import torch\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Feature extractor\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
        "    \"facebook/hubert-base-ls960\"\n",
        ")\n",
        "\n",
        "# Load HuBERT with hidden states enabled\n",
        "hubert = HubertModel.from_pretrained(\n",
        "    \"facebook/hubert-base-ls960\",\n",
        "    output_hidden_states=True   # ⭐ IMPORTANT\n",
        ").to(DEVICE)\n",
        "\n",
        "hubert.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0845993ed4a74a8dabb2471625dfd874",
            "3d1de51a23f44aae94c0add3b0c391be",
            "3f503341fa7145419d96b87c144b3561",
            "f3ac6da5ce46432fbc72f47b05af833e",
            "c39dbdee4b5d40ba825f20c3b9c54b17",
            "b5b9b95c8e0441859e0ba4b42a05c68c",
            "15c08c48745b4774949ff842c65030cd",
            "506b1cbff9cd4774861f2beba021b5d7",
            "c6b45e828a954bdaa6e13f3cac8cbbc8",
            "d7d219744a704347bea950acb7b9467a",
            "40ea10ae75d543f1b2f1a5f57cef24ce"
          ]
        },
        "id": "1J-ZmZ-2_Qnj",
        "outputId": "6996a94a-eee9-406c-d215-eec5354ef8ab",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/211 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0845993ed4a74a8dabb2471625dfd874"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HubertModel(\n",
              "  (feature_extractor): HubertFeatureEncoder(\n",
              "    (conv_layers): ModuleList(\n",
              "      (0): HubertGroupNormConvLayer(\n",
              "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (1-4): 4 x HubertNoLayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (5-6): 2 x HubertNoLayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (feature_projection): HubertFeatureProjection(\n",
              "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): HubertEncoder(\n",
              "    (pos_conv_embed): HubertPositionalConvEmbedding(\n",
              "      (conv): ParametrizedConv1d(\n",
              "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "        (parametrizations): ModuleDict(\n",
              "          (weight): ParametrizationList(\n",
              "            (0): _WeightNorm()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (padding): HubertSamePadLayer()\n",
              "      (activation): GELUActivation()\n",
              "    )\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x HubertEncoderLayer(\n",
              "        (attention): HubertAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): HubertFeedForward(\n",
              "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in hubert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "ztG_7E8WdRi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_emotion(e):\n",
        "    if e is None:\n",
        "        return None\n",
        "\n",
        "    e = e.strip().lower()\n",
        "\n",
        "    if e in [\"pleasant_surprised\", \"pleasant_surprise\"]:\n",
        "        return \"pleasant_surprise\"\n",
        "\n",
        "    return e\n"
      ],
      "metadata": {
        "id": "nzffOUNm_i8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    for folder in tqdm(os.listdir(DATASET_PATH), desc=\"Emotions\"):\n",
        "\n",
        "        folder_path = os.path.join(DATASET_PATH, folder)\n",
        "\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        # Extract emotion label\n",
        "        emotion = normalize_emotion(folder.split(\"_\", 1)[1].lower())\n",
        "\n",
        "        save_dir = os.path.join(FEATURE_SAVE_PATH, emotion)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        wav_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
        "\n",
        "        for wav_file in tqdm(wav_files, desc=f\"Extracting {emotion}\", leave=False):\n",
        "\n",
        "            wav_path = os.path.join(folder_path, wav_file)\n",
        "            save_path = os.path.join(\n",
        "                save_dir,\n",
        "                wav_file.replace(\".wav\", \".pt\")\n",
        "            )\n",
        "\n",
        "            if os.path.exists(save_path):\n",
        "                continue\n",
        "\n",
        "            # -------------------------------------------------\n",
        "            # LOAD AUDIO\n",
        "            # -------------------------------------------------\n",
        "            waveform, sr = torchaudio.load(wav_path)\n",
        "            waveform = waveform.squeeze(0)\n",
        "\n",
        "            if sr != SAMPLE_RATE:\n",
        "                waveform = torchaudio.functional.resample(\n",
        "                    waveform, sr, SAMPLE_RATE\n",
        "                )\n",
        "\n",
        "            # -------------------------------------------------\n",
        "            # FEATURE EXTRACTOR\n",
        "            # -------------------------------------------------\n",
        "            inputs = feature_extractor(\n",
        "                waveform.numpy(),\n",
        "                sampling_rate=SAMPLE_RATE,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_values = inputs.input_values.to(DEVICE)\n",
        "\n",
        "            # -------------------------------------------------\n",
        "            # HUBERT FORWARD PASS\n",
        "            # -------------------------------------------------\n",
        "            outputs = hubert(input_values)\n",
        "\n",
        "            hidden_states = outputs.hidden_states\n",
        "\n",
        "            # Layers 5–9\n",
        "            selected_layers = hidden_states[5:10]\n",
        "\n",
        "            # Stack → (5, B, T, 768)\n",
        "            stacked = torch.stack(selected_layers)\n",
        "\n",
        "            # Mean fusion → (B, T, 768)\n",
        "            features = torch.mean(stacked, dim=0)\n",
        "\n",
        "            # Remove batch dimension → (T, 768)\n",
        "            features = features.squeeze(0).cpu()\n",
        "\n",
        "            # -------------------------------------------------\n",
        "            # SAVE FEATURES\n",
        "            # -------------------------------------------------\n",
        "            torch.save(features, save_path)\n",
        "\n",
        "print(\"✅ Feature extraction completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojgjff1tegBL",
        "outputId": "765ef0a3-dbf4-47a3-a7aa-c3134fb11580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Emotions:   0%|          | 0/14 [00:00<?, ?it/s]\n",
            "Extracting fear:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                        \u001b[A\n",
            "Extracting angry:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                         \u001b[A\n",
            "Extracting fear:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "Emotions:  21%|██▏       | 3/14 [00:00<00:00, 23.67it/s]\n",
            "Extracting disgust:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                           \u001b[A\n",
            "Extracting neutral:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                           \u001b[A\n",
            "Extracting angry:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "Emotions:  43%|████▎     | 6/14 [00:00<00:00, 23.41it/s]\n",
            "Extracting sad:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                       \u001b[A\n",
            "Extracting disgust:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                           \u001b[A\n",
            "Extracting neutral:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "Emotions:  64%|██████▍   | 9/14 [00:00<00:00, 22.92it/s]\n",
            "Extracting pleasant_surprise:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                     \u001b[A\n",
            "Extracting happy:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                         \u001b[A\n",
            "Extracting happy:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "Emotions:  86%|████████▌ | 12/14 [00:00<00:00, 22.92it/s]\n",
            "Extracting sad:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "                                                       \u001b[A\n",
            "Extracting pleasant_surprise:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "Emotions: 100%|██████████| 14/14 [00:00<00:00, 22.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Feature extraction completed successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = torch.load(\"/content/drive/MyDrive/Colab Notebooks/hubert_features/angry/OAF_back_angry.pt\")\n",
        "print(sample.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hnxud-fe5em",
        "outputId": "062480f4-0883-4cb7-934d-d189377dbd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([76, 768])\n"
          ]
        }
      ]
    }
  ]
}